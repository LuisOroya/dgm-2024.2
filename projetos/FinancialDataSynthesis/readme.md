# `S√≠ntese de Dados Financeiros`
==============================

# `Financial Data Synthesis`
==============================
## Link dos slides:
https://docs.google.com/presentation/d/1eOmgRpkQeXU1htM_7Gq66HRcn2CPZ7iB/edit?pli=1#slide=id.p1

Apresenta√ß√£o
==============================
<p align="justify">
O presente projeto foi originado no contexto das atividades da disciplina de p√≥s-gradua√ß√£o IA376N - Deep Learning aplicado a S√≠ntese de Sinais, oferecida no segundo semestre de 2024, na Unicamp, sob supervis√£o da Profa. Dra. Paula Dornhofer Paro Costa, do Departamento de Engenharia de Computa√ß√£o e Automa√ß√£o (DCA) da Faculdade de Engenharia El√©trica e de Computa√ß√£o (FEEC).
</p>

 |Nome  | RA | Curso|
 |--|--|--|
 |Jos√© Carlos Ferreira  | 170860  | Eng. El√©trica |
 |Byron Alejandro Acu√±a Acurio  | 209428  | Eng. El√©trica |

## Resumo (Abstract)
<p align="justify">
 
Neste trabalho, estudamos a efic√°cia de modelos baseados em **Redes Advers√°rias Generativas (GANs)** e **Transformers** na gera√ß√£o de dados financeiros sint√©ticos, especificamente pre√ßos de a√ß√µes. As GANs s√£o redes neurais que podem gerar novos dados realistas a partir de um conjunto de treinamento, enquanto os Transformers se destacam em tarefas que envolvem sequ√™ncias temporais devido √† sua capacidade de capturar depend√™ncias de longo alcance.

Utilizamos s√©ries temporais de pre√ßos de a√ß√µes e indicadores t√©cnicos como **entrada** para os modelos, com o objetivo de realizar regress√£o para prever pre√ßos futuros. Para treinar os modelos, dividimos o dataset em sequ√™ncias de 24 pre√ßos consecutivos, associando o 25¬∫ pre√ßo como target (pre√ßo a ser predito). Essa estrat√©gia permite que os modelos aprendam padr√µes dentro de janelas temporais espec√≠ficas, ao inv√©s de analisar a s√©rie temporal inteira de uma s√≥ vez.

Observamos que, ao utilizar sequ√™ncias de tamanho fixo, os modelos conseguiram extrair padr√µes de forma mais eficiente e gerar previs√µes mais precisas, em compara√ß√£o com estrat√©gias que consideram a s√©rie temporal completa como entrada.

Comparamos os resultados dos nossos modelos baseados em GANs e Transformers com modelos de aprendizado profundo tradicionais, como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Unit), utilizando a m√©trica RMSE (Root Mean Squared Error) para avaliar a precis√£o. Os resultados indicam que os modelos propostos oferecem vantagens significativas na previs√£o de pre√ßos de a√ß√µes.
</p>

## Introdu√ß√£o
<p align="justify">

A previs√£o de pre√ßos de a√ß√µes √© uma tarefa importante na √°rea de finan√ßas, com aplica√ß√µes que v√£o desde a negocia√ß√£o algor√≠tmica (trading) at√© a gest√£o de riscos [1]. Modelos tradicionais de previs√£o, como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Unit), s√£o  amplamente utilizados para essa finalidade. No entanto, a natureza complexa dos mercados financeiros pode requerer abordagens mais sofisticadas, que possam capturar padr√µes n√£o lineares e depend√™ncias de longo alcance nas s√©ries temporais.

Neste trabalho, exploramos o uso de modelos baseados em Redes Advers√°rias Generativas (GANs) e Transformers para melhorar a qualidade das previs√µes de pre√ßos de a√ß√µes. Embora as GANs sejam tradicionalmente utilizadas para a gera√ß√£o de imagens sint√©ticas, elas podem ser adaptadas para tarefas de predi√ß√£o ao aprender representa√ß√µes profundas dos dados. Os Transformers, por sua vez, t√™m se mostrado eficazes em tarefas sequenciais devido √† sua capacidade de modelar rela√ß√µes complexas em s√©ries temporais.

Neste estudo, realizamos experimentos utilizando os pre√ßos das a√ß√µes da empresa Apple no per√≠odo de 2010 a 2020, per√≠odo que inclui o evento extremo da pandemia de COVID-19. Isso nos permitiu avaliar a robustez dos modelos generativos em condi√ß√µes de alta volatilidade e incertezas.

Uma s√©rie temporal de pre√ßos de a√ß√µes pode ser representada como:

$$ X_{1:N} = [x(1), x(2), ..., x(N)] $$

- $x(i)$: representa o pre√ßo da a√ß√£o no dia $i$ (pre√ßo de fechamento).

Al√©m dos pre√ßos hist√≥ricos, incorporamos features relevantes, como indicadores t√©cnicos, tamb√©m organizados em s√©ries temporais:

$$ F_{1:N} = [f(1), f(2), ..., f(N)] $$

Nosso objetivo √© utilizar essas informa√ß√µes para gerar uma continua√ß√£o realista da s√©rie temporal de pre√ßos, resultando em uma nova s√©rie sint√©tica:

$$ X^{s}_{N+1:N+K} = [x^{s}(N+1), x^{s}(N+2), ..., x^{s}(N+K)] $$

Desejamos que essa s√©rie sint√©tica seja uma aproxima√ß√£o da sequ√™ncia real futura:

$$ X^{s}_{N+1:N+K} \approx X\_{N+1:N+K} $$

Por exemplo, se  $X_{1:N}$ representa os pre√ßo de uma a√ß√£o de 2010 at√© 2018, ent√£o desejamos que $X_{N+1:N+K}$ forne√ßa valores plaus√≠veis de pre√ßo de 2018 em diante. 
</p>

## Descri√ß√£o do Problema/Motiva√ß√£o
<p align="justify">
No setor financeiro, o acesso a dados do mundo real para an√°lise e treinamento de modelos √© limitado devido a quest√µes de privacidade e seguran√ßa. Assim os dados sint√©ticos podem ajudar a fornecer uma alternativa segura para disponibilizar esses dados para diversas organiza√ß√µes. O desenvolvimento de modelos com capacidade de prever o pre√ßo da a√ß√£o de forma precisa √© desafiador devido √† complexidade inerente desses dados. Em geral, os dados financeiros s√£o n√£o estacion√°rios e seguem distribui√ß√µes de probabilidade desconhecidas e dif√≠ceis de serem estimadas. Apesar dos avan√ßos nos algoritmos de deep learning, que conseguem capturar melhor essas complexidades, a escassez de dados financeiros dispon√≠veis tem sido um fator limitante na constru√ß√£o de m√©todos robustos. Especialmente em eventos extremos quando no hist√≥rico de dados nunca se teve um registro de um evento similar.
</p>

<p align="justify">
H√° um movimento crescente entre pesquisadores para otimizar modelos de machine learning atrav√©s da incorpora√ß√£o de dados financeiros sint√©ticos [4]. A gera√ß√£o de dados sint√©ticos permite melhorar o desempenho de m√©todos que, at√© ent√£o, apresentavam resultados insatisfat√≥rios ou eram invi√°veis na pr√°tica devido √† falta de dados, al√©m de possibilitar a simula√ß√£o de eventos raros ou extremos. 
</p>

<p align="justify">
Diversas metodologias t√™m sido estudadas. As arquiteturas da fam√≠lia Generative Adversarial Networks (GANs) t√™m mostrado bons resultados em tarefas de gera√ß√£o de imagens e, mais recentemente, est√£o sendo aplicadas na gera√ß√£o de dados financeiros sint√©ticos. Al√©m das GANs, as arquiteturas Transformers tamb√©m surgem como estruturas promissoras para a tarefa. 
</p>

<p align="justify">
A cria√ß√£o de dados financeiros que reproduzam o comportamento de dados reais √© essencial para v√°rias aplica√ß√µes, como o problema de otimiza√ß√£o de portf√≥lios. Considere um investidor com acesso a ùëõ classes de ativos. O problema de otimiza√ß√£o de portf√≥lio consiste em alocar esses ativos de modo a maximizar o retorno, escolhendo a quantidade apropriada para cada classe, enquanto mant√©m o risco do portf√≥lio dentro de um n√≠vel de toler√¢ncia predefinido. Pesquisas recentes em otimiza√ß√£o de portf√≥lios financeiros exploraram diversas abordagens para melhorar as estrat√©gias de aloca√ß√£o de ativos. A gera√ß√£o de dados sint√©ticos tem se destacado como uma boa solu√ß√£o para ampliar conjuntos de dados financeiros limitados, com estudos propondo modelos de regress√£o sint√©tica [1] e redes advers√°rias generativas condicionais modificadas [2].
</p>

<p align="justify">
Neste trabalho, exploramos o uso de modelos baseados em Redes Advers√°rias Generativas (GANs) e Transformers para melhorar a qualidade das previs√µes de pre√ßos de a√ß√µes. As GANS podem ser adaptadas para tarefas de predi√ß√£o ao aprender representa√ß√µes profundas dos dados, enquanto os Transformers s√£o eficazes em modelar rela√ß√µes n√£o lineares em s√©ries temporais.
</p>

## Objetivos

O projeto teve como objetivos:

-  Estudar e desenvolver dois modelos generativos baseados nas arquiteturas GANs e Transformers para predizer pre√ßos de a√ß√µes baseados em valores hist√≥ricos e features relevantes.
- Comparar o desempenho desses modelos com modelos tradicionais de s√©ries temporais, como as redes neurais recorrentes LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Unit).

Para alcan√ßar esses objetivos, utilizamos:
- O hist√≥rico dos pre√ßos di√°rios (pre√ßo de fechamento) das a√ß√µes da empresa Apple Inc. (ticker: AAPL), em d√≥lares, no per√≠odo de 1¬∫ de julho de 2010 at√© 1¬∫ de julho de 2020.

## Contribui√ß√µes
- Cria√ß√£o de um modelo generativo baseado em GAN;  
- Cria√ß√£o de um modelo generativo baseado em Wasserstein GAN;  
- Cria√ß√£o de um modelo generativo baseado em Transformers;
- Compara√ß√£o de desempenho com redes profundas baseadas em LSTM e GRU;

Utilizamos como m√©trica de avalia√ß√£o o Root Mean Square Error (RMSE).

### Bases de Dados

|Base de Dados | Endere√ßo na Web | Resumo descritivo|
|----- | ----- | -----|
|API do Yahoo Finance| https://finance.yahoo.com | Permite o acesso a dados financeiros por meio de chamadas de API. Esses dados incluem pre√ßos de fechamento, pre√ßos m√°ximos, m√≠nimos, volume negociado. Al√©m disso, √© poss√≠vel coletar os dados considerando diferentes per√≠odos de amostragem: 2 minutos, 5 minutos, 15 minutos, 1 hora, 1 dia.|

## Metodologia e Workflow
**CASO 1: GANs**

A metodologia para a gera√ß√£o das s√©ries temporais sint√©ticas utilizando modelos baseados na arquitetura das Redes Generativas Adversarias pode ser resumida no seguinte passo a passo:

1. **Coleta de Dados:**

<p align="justify">
Para os modelos baseados em GAN, al√©m dos pre√ßos hist√≥ricos, coletamos dados adicionais que servem como features, como √≠ndices de mercado, pre√ßos de commodities e a√ß√µes de outras grandes empresas do setor de tecnologia, como Amazon, Google e Microsoft.

O per√≠odo analisado inclui 2020, escolhido intencionalmente para testar os modelos em um cen√°rio de alta volatilidade e incertezas causado pela pandemia de Covid-19.
</p>

</p>
<div align="center">
    <img src="img_readme/Banco de Dados.png" alt="Banco de Dados" title="Banco de Dados" />
    <p><em>Figura 1: Banco de Dados Analisado para os modelos baseados em GAN.</em></p>
</div>


2. **Engenharia de Features:**

<p align="justify">
Depois de baixar os hist√≥ricos de dados das s√©ries temporais financeiras, calculamos alguns indicadores t√©cnicos e extra√≠mos algumas caracter√≠sticas de tend√™ncia. Al√©m disso, foram criadas features baseadas em transformadas de Fourier para extrair tend√™ncias de longo e curto prazo nos pre√ßos das a√ß√µes.
</p>

<ul>
  <li>Indicadores t√©cnicos: M√©dia m√≥vel de 7 e 20 dias, m√©dia m√≥vel exponencial, momentum, bandas de Bollinger, MACD.</li>
  <li>Transformadas de Fourier: Foi obtida a magnitude e a fase das transformadas discretas de Fourier do pre√ßo das a√ß√µes, usando 3, 6 e 9 componentes.</li>
</ul>

</p>
<div align="center">
    <img src="img_readme/Tecnicos.png" alt="Features Baseadas em Indicadores T√©cnicos" title="Indicadores T√©cnicos" />
    <p><em>Figura 2: Features Baseadas em Indicadores T√©cnicos criadas como complemento para o banco de dados apresentado na Figura 1.</em></p>
</div>

<div align="center">
    <img src="img_readme/Fourier.png" alt="Features Baseadas em Transformadas de Fouerier" title="Transformadas de Fouerier" />
    <p><em>Figura 3: Features Baseadas em Transformadas de Fouerier criadas como complemento para o banco de dados apresentado na Figura 1.</em></p>
</div>

3. **Normaliza√ß√£o dos Dados:**

Ap√≥s a coleta dos pre√ßos $X_{1:N}$ e features $F_{1:N}$, armazenamos as s√©ries temporais em um mesmo dataframe: 

$$D = [X_{1:N}, F_{1:N}]$$

Para facilitar o treinamento do modelo, os valores, para cada s√©rie temporal, foram normalizados utilizando a t√©cnica min-max. A f√≥rmula adotada foi:

$$
x_{n}(i) = \frac{x(i) - \min(x)}{\max(x) - \min(x)}
$$

- $x_{n}(i)$: representa o valor normalizado (pre√ßo ou feature) no instante $i$.
- $x(i)$: representa o valor original (pre√ßo ou feature) no instante $i$.
- $\min(x)$: representa o menor valor na s√©rie temporal $x$.
- $\max(x)$: representa o maior valor na s√©rie temporal $x$.

Essa abordagem garante que os valores da s√©rie temporal sejam escalados para o intervalo $[0, 1]$, permitindo que o modelo processe os dados de forma eficiente e consistente.

4. **Constru√ß√£o da Rede Neural:**

<p align="justify">
O modelo GAN para predi√ß√£o de s√©ries temporais, usa como fun√ß√£o de perda a diverg√™ncia de Kullback-Leibler (KL) e a diverg√™ncia de Jensen-Shannon (JS), no processo de treinamento. Essas duas diverg√™ncia s√£o m√©tricas matem√°ticas usadas para medir a semelhan√ßa entre duas distribui√ß√µes de probabilidade. O modelo GAN usa no Discriminador a fun√ß√£o de perda baseada na diverg√™ncia de Jensen-Shannon (JS), que se apresenta a continua√ß√£o:

$$
-\frac{1}{m} \sum_{i=1}^m \log D\left(y^i\right)-\frac{1}{m} \sum_{i=1}^m\left(1-\log D\left(G\left(x^i\right)\right)\right)
$$


Neste projeto, o gerador foi treinado usando perda de entropia cruzada para minimizar a diferen√ßa entre duas distribui√ß√µes, o que equivale a minimizar a diverg√™ncia de Kullback-Leibler (KL), usando a fun√ß√£o de perda apresentada a continua√ß√£o

$$
-\frac{1}{m} \sum_{i=1}^m\left(\log D\left(G\left(x^i\right)\right)\right)
$$
 
 
 A rede √© composta por duas redes neurais: (i) Gerador e (ii) Discriminador. As fun√ß√µes de perda apresentadas anteriormente garantem um treinamento que combina um processo competitivo. Abaixo detalha-se a estrutura das redes neurais da GAN:
</p>
<div align="center">
    <img src="img_readme/GAN.png" alt="Estrutura GAN" title="Estrutura da rede generativa GAN" />
    <p><em>Figura 4: Estrutura da arquitetura GAN.</em></p>
</div>


<p align="justify">
O gerador foi constru√≠do usando uma sequ√™ncia de tr√™s camadas GRU (Gated Recurrent Unit) para processamento de dados sequenciais e tr√™s camadas densas para refinar os resultados e produzir o dado sint√©tico final. A sele√ß√£o das tr√™s camadas GRU foi porque o modelo utilizou os ultimos tr√™s dias de dados hist√≥ricos para poder prever o pre√ßo do dia seguente. Na entrada, temos as 36 features explicadas anteriormente nos passos 1 e 2. Por isso, observamos que temos uma dimens√£o de entrada de (Bs,3,36), em que Bs √© o tamanho do batch de treinamento. No estudo,  consideramos Bs = 128. Note-se que a GAN usada tem uma arquitetura condicional, onde a gera√ß√£o dos dados √© condicionada a alguns dados de entrada. Neste caso, o contexto usado foi os valores passados reais do valor da a√ß√£o da Apple $yc$. 
</p>

<p align="justify">
Adicionalmente no caso das GANs, neste estudo foi explorada o tipo de GAN conhecidas na literatura como Wasserstein GAN com Gradient Penalty (WGAN-GP), que oferece vantagens significativas sobre as GANs padr√£o para gera√ß√£o de s√©ries temporais devido √† sua estabilidade de treinamento aprimorada e capacidade de mitigar desafios comuns, como colapso de modo. Por exemplo as GANs padr√£o, que dependem de perda de entropia cruzada bin√°ria, muitas vezes enfrentam gradientes inst√°veis, particularmente em cen√°rios onde o discriminador domina o gerador, levando a uma din√¢mica de aprendizagem abaixo do ideal. Este problema √© agravado em dados de s√©ries temporais, onde as depend√™ncias temporais e os padr√µes complexos aumentam a dificuldade de alcan√ßar um processo de forma√ß√£o equilibrado. O WGAN-GP aborda essas limita√ß√µes usando a perda de Wasserstein, que mede a diverg√™ncia entre distribui√ß√µes de dados reais e geradas por meio da Dist√¢ncia do Earth Mover, garantindo atualiza√ß√µes de gradiente suaves e significativas mesmo quando o discriminador funciona bem. Al√©m disso, a penalidade de gradiente imp√µe uma restri√ß√£o de Lipschitz ao discriminador sem recorrer ao corte de peso, melhorando a capacidade do discriminador de modelar as estruturas intrincadas inerentes aos dados de s√©ries temporais. Essas melhorias n√£o apenas promovem um treinamento est√°vel, mas tamb√©m reduzem o risco de colapso do modo, incentivando o gerador a produzir padr√µes de s√©ries temporais diversos e realistas. Ao alinhar mais estreitamente a distribui√ß√£o dos dados gerados com a distribui√ß√£o real, o WGAN-GP surge como uma escolha robusta para tarefas de modelagem de s√©ries temporais, permitindo a s√≠ntese de sequ√™ncias de alta qualidade com caracter√≠sticas temporais complexas. A continua√ß√£o se apresenta na Figura 5 as principais diferen√ßas entre a abordagem GAN e WGAN-GP
</p>

<div align="center">
    <img src="img_readme/WGAN-GP.png" alt="Diferen√ßas da GAN com WGAN-GP" title="Diferen√ßas da GAN com WGAN-GP" />
    <p><em>Figura 5: Diferen√ßas da GAN com WGAN-GP.</em></p>
</div>

Assim temos que a fun√ß√£o de perda do discriminante na WGAN-GP (Wasserstein GAN com Gradient Penalty) foi projetada para aproximar a dist√¢ncia de Wasserstein entre distribui√ß√µes de dados reais e geradas. Consiste dos seguentes componentes principais:

<div align="center">
    <img src="img_readme/ComponentesWGAN-GP.png" alt="Componentes da WGAN-GP" title="Componentes da WGAN-GP" />
    <p><em>Figura 6: Componentes da WGAN-GP.</em></p>
</div>

**CASO 2: TRANSFORMERS**

A metodologia para a gera√ß√£o das s√©ries temporais sint√©ticas utilizando arquitetura Transformers pode ser resumida no seguinte passo a passo:

1. **Coleta de Dados via API do Yahoo Finance:**
   
   Atrav√©s desse API, coletamos os pre√ßos com um per√≠odo de amostragem de 2 minutos, e armazenamos em um vetor que representa a s√©rie temporal: $X\_{1:N}$.
   
   O per√≠odo de amostragem de 2 minutos foi escolhido pois √© o menor que o API disponibiliza. Optamos por realizar uma an√°lise em alta frequ√™ncia, pois as varia√ß√µes nos pre√ßos n√£o s√£o t√£o abruptas comparadas √† de uma frequ√™ncia menor (e.g. valores di√°rios). Dessa forma, o modelo consegue gerar dados dentro de uma faixa razo√°vel de valores. A figura abaixo ilustra um exemplo.
   
<p align="justify">
A continua√ß√£o se apresenta a serie temporal dos pre√ßos da a√ß√£o da empressa Apple, a usada data usada dos dados foi desde 2010-07-01 at√© 2020-06-30, para fazer experimentos antes e depois do Covid-19 (evento extremo)
</p>

<div align="center">
    <img src="img_readme/Serie_temporal.png" alt="Pre√ßos_Vale" title="Pre√ßos Apple" />
    <p><em>Figura 7: Pre√ßos das a√ß√µes da Apple com um per√≠odo de amostragem de 2 minutos coletados do API do Yahoo Finance.</em></p>
</div>

2. **Extra√ß√£o de Features:**

   Para auxiliar na gera√ß√£o de dados sint√©ticos realistas, tamb√©m extraimos diversos features que ajudam a explicar o comportamento dos pre√ßos. Esses features tamb√©m s√£o s√©ries temporais, dados (cada um) por: $F\_{1:N}$. Eles possuem o mesmo n√∫mero de amostras da s√©rie temporal de pre√ßos.

Os features que se mostraram √∫teis na gera√ß√£o dos dados sint√©ticos foram:

   - Volume de a√ß√µes negociada;
   - √çndices t√©cnicos: Moving Average Convergence Divergence (MACD), Stochastic Oscillator (SO), Commodity Channel Index (CCI), Money Flow Index (MFI);
  
Os √≠ndices t√©cnicos s√£o algumas m√©tricas que podem ser calculadas a partir do pre√ßo de fechamento, pre√ßo m√°ximo e m√≠nimo, al√©m do volume de a√ß√µes negociadas. Esses √≠ndices t√©cnicos buscam capturar as tend√™ncias de movimenta√ß√£o dos pre√ßos. A figura abaixo ilustra um exemplo de um feature utilizado:

<div align="center">
    <img src="img_readme/vol.png" alt="Volume_Vale" title="Volume de A√ß√µes da Apple" />
    <p><em>Figura 8: Volume de a√ß√µes da Apple negociadas com um per√≠odo de amostragem de 2 minutos coletados do API do Yahoo Finance.</em></p>
</div>

3. **Normaliza√ß√£o dos Dados:**

   Ap√≥s a coleta dos dados e extra√ß√£o dos features, armazenamos as s√©ries temporais (do pre√ßo e dos features) em um mesmo dataframe: $D=[X\_{1:N}, F\_{1:N} ]$.
   
   Ap√≥s isso, normalizamos os valores de cada s√©rie temporal para facilitar o treinamento, utilizando as suas respectivas m√©dias e desvios padr√µes. A normaliza√ß√£o adotada foi:

$$ x_{n}(i) = \frac{x(i) - \text{m√©dia[x]}}{\text{desvio padr√£o[x]}}$$

- $x_{n}(i)$: representa o valor normalizado de uma s√©rie temporal (pre√ßo ou algum feature) no instante $i$.
-  $x(i)$: representa o valor antes da normaliza√ß√£o (pre√ßo ou algum feature) no instante $i$.
- m√©dia[x], desvio padr√£o [x] : representam a m√©dia e o desvio padr√£o associado √† s√©rie temporal dos elementos de x(i)  

   
4. **Constru√ß√£o da Rede Neural:**

   A rede neural √© um modelo baseado na arquitetura Transformer sendo utilizado para predi√ß√£o de s√©ries temporais. Ele processa sequ√™ncias de dados para predizer o valor futuro com base nas observa√ß√µes passadas. A figura abaixo ilustra o modelo, de maneira simplificada, atr√°ves de blocos:
   <div align="center">
    <img src="Arquitetura_Blocos.png" alt="Arquitetura" title="Arquitetura" />
    <p><em>Figura 9: Estrutura simplificada do modelo baseado na arquitetura Transformer. </em></p>
</div>

- **Input:**
   
   A entrada √© um dataframe D contendo a s√©rie temporal do pre√ßo $X_{1:N}$ e dos features $F\_{1:N}$.
   
- **Sequenciador das S√©ries Temporais:**
   
   As s√©ries temporais s√£o repartidas em sequ√™ncias de tamanho fixo (tam_seq) para o processamento nos blocos Transformers. Al√©m disso, associamos a cada sequ√™ncia um target, que representa o valor que desejamos prever (r√≥tulo). Para o treinamento, a rede recebe um conjunto de sequ√™ncias e os r√≥tulos correspondentes.
   
- **Layer de Input:**
   
   A entrada da rede √© um vetor multidimensional que cont√©m todas as sequ√™ncias de tamanho tam_seq para todos os features.
   
- **Embedding Layer:**

   A embedding layer √© uma camada densa que transforma os dados em um espa√ßo dimensional maior. √â √∫til para que o modelo aprenda rela√ß√µes mais complexas nos dados.

- **Positional Encoding:**

   Adiciona informa√ß√µes sobre a posi√ß√£o de cada elemento da sequ√™ncia, visto que o Transformer n√£o conhece a ordem temporal dos dados. Isso permite que o modelo saiba a ordem temporal dos dados.

- **Blocos Transformers:**

   Sequ√™ncias de blocos da arquitetura Transformer, cada bloco possui os seguintes elementos:

   - Layer MultiHead Attention: permite que o modelo se concentre em diferentes partes da sequ√™ncia para realizar a predi√ß√£o
   - Conex√£o Residual e Normaliza√ß√£o: adiciona a entrada do bloco √† sa√≠da do layer MultiHead Attention e normaliza os valores. Isso ajuda na estabiliza√ß√£o de treinamento.
   - Rede Feed-Forward: duas camadas densas com fun√ß√£o de ativa√ß√£o ReLU na primeira.
     
- **Global Average Pooling:**
    
   Reduz a sa√≠da dos blocos transformers para um vetor de tamanho fixo atrav√©s do c√°lculo da m√©dia dos valores.

- **Output Layer**:

    Camada densa que gera o valor predito. No nosso modelo, predizemos apenas um √∫nico valor por vez.

Os detalhes espec√≠ficos da constitui√ß√£o de cada bloco est√£o descritos neste link: [Detalhes_Arquitetura](docs/Arquitetura.md)

5. **Treinamento:**

Ap√≥s a constru√ß√£o do modelo, partimos para a etapa de treinamento. Nesta etapa, o nossos dados de entrada $D = [X_{1:N}, F_{1:N}]$ s√£o separados em conjunto de treinamento, valida√ß√£o e teste:

- Conjunto de treinamento: Os 70% primeiros elementos do dataset de entrada
- Conjunto de valida√ß√£o:      20% dos elementos do dataset
- Conjunto de teste:       Os 10% √∫ltimos elementos do dataset de entrada

Por exemplo, se o dataset de entrada s√£o s√©ries temporais com 1000 elementos, ent√£o os 700 primeiros elementos s√£o utilizados para treinamento, os 200 elementos seguintes para valida√ß√£o, e os √∫ltimos 100 para teste. Foi importante garantir que os dados estejam ordenados, pois apresentam depend√™ncias temporais.

Conforme explicado no bloco de sequenciamento das s√©ries temporais, os dados s√£o transformados em sequ√™ncias de tamanho fixo. No nosso caso, observamos que sequ√™ncias com 24 instantes de tempo consecutivos apresentaram os melhores resultados. Logo, o modelo recebe como entrada sequ√™ncias com 24 elementos consecutivos e o r√≥tulo associado, que no caso, seria o 25¬∫ elemento.

Ou seja, dado os √∫ltimos 24 pre√ßos (e features), o modelo tentar√° prever o 25¬∫ pre√ßo, e a verifica√ß√£o da qualidade da solu√ß√£o ser√° dado pela compara√ß√£o com o valor do r√≥tulo que √© o valor real do 25¬∫ pre√ßo.

Para o treinamento, foi utilizado os seguintes hiperpar√¢metros:
- Otimizador: Adaptative Moment Estimator (Adam);
- Fun√ß√£o de perda: Mean Absolute Error;
-  Batch size: 128;
-  N√∫mero de √©pocas: 200 (com early stopping);

  A escolha dos melhores par√¢metros foi baseado na perda observada para o conjunto de valida√ß√£o.

  6. **Infer√™ncia:**

Ap√≥s o treinamento, utilizamos o modelo para prever os pontos do conjunto de teste e comparamos com os respectivos r√≥tulos associados.

A figura abaixo ilustra o workflow:

 <div align="center">
    <img src="Workflow.png" alt="Workflow" title="Workflow" />
    <p><em>Figura 10: Workflow contemplando o processo de treinamento e infer√™ncia. </em></p>
</div>

## Experimentos, Resultados e Discuss√£o dos Resultados

### Avalia√ß√£o Quantitativa: Root Mean Square Error


| Model                                      | LSTM | GRU  | GAN  | WGAN-GP  | Transformer  |
|--------------------------------------------|------|------|------|----------|--------------|
| RMSE do conjunto de treinamento            | 1.52 | 6.60 | 9.45 |   1.74   |   0.52       |
| RMSE do conjunto de teste (incluindo 2020) | 6.60 | 5.33 | 4.08 |   4.77   |   2.605      |
| RMSE do conjunto de teste (excluindo 2020) | 9.45 | 5.38 | 3.09 |   3.88   |   1.976      |

### Avalia√ß√£o Qualitativa das GANs
A figura permite fazer a inspe√ß√£o visual das s√©ries temporais geradas pelos modelos GANs

 <div align="center">
    <img src="img_readme/Qualitativa.png" alt="Qualitativa" title="Qualitativa" />
    <p><em>Figura 11: Avalia√ß√£o Qualitativa das GANs. </em></p>
</div>

### Artigos de Refer√™ncia
Os principais artigos que o grupo j√° identificou como base para estudo e planejamento do projeto s√£o:

- **Pagnocelli. (2022)**: "A Synthetic Data-Plus-Features Driven Approach for Portfolio Optimization" [5].
  
- **Pe√±a et al. (2024)**: "A modified CTGAN-plus-features-based method for optimal asset allocation" [2].

-  **F.Eckerli, J.Osterrieder.** "Generative Adversarial Networks in finance: an overview" [3]. 

### Ferramentas
- **Python** com bibliotecas como `PyTorch` e `scikit-learn` para implementar os modelos generativos e realizar a s√≠ntese de dados.
   
- **Colab** para colabora√ß√£o e execu√ß√£o de experimentos em ambientes com suporte a GPU.
  
- **Pandas** e **NumPy** para manipula√ß√£o de dados tabulares.

## Conclus√£o
Por fim, a principal dificuldade do projeto ser√° gerar os dados financeiros sint√©ticos realistas. Abordaremos diversas estrat√©gias que v√£o desde o pr√©-processamento dos dados, ajustes nos hiperpar√¢metros das GANs e o emprego de m√©tricas eficientes.
 
## Refer√™ncias Bibliogr√°ficas
[1] Li, Gaorong, Lei Huang, Jin Yang, and Wenyang Zhang.  
"A synthetic regression model for large portfolio allocation."  
*Journal of Business & Economic Statistics* 40, no. 4 (2022): 1665-1677.

[2] Pe√±a, Jos√©-Manuel, Fernando Su√°rez, Omar Larr√©, Domingo Ram√≠rez, and Arturo Cifuentes. 
"A modified CTGAN-plus-features-based method for optimal asset allocation".
" Quantitative Finance 24, no. 3-4 (2024): 465-479".

[3] https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html

[4] F.Eckerli, J.Osterrieder.
" Generative Adversarial Networks in finance: an overview."

[5]- Bernardo K. Pagnoncelli, Arturo Cifuentes, Domingo Ram√≠rez and Hamed Rahimian.
 "A Synthetic Data-Plus-Features Driven Approach for Portfolio Optimization".
 Computational Economics, 2023, Volume 62, Number 1, Page 187.


Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ make_dataset.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ build_features.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ   ‚îÇ                 predictions
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
