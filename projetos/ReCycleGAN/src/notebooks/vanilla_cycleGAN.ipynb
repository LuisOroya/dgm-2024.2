{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMh5APzXGI0j"
      },
      "source": [
        "# Implementing the CycleGAN (vanilla architecture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVYYbaJRGCWv"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "Reproduzir uma CycleGAN vanilla, baseada no artigo [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593v7).\n",
        "\n",
        "Mais especificamente, construir, treinar e documentar esta arquitetura de GAN utilizando Pytorch, baseado na implementação em [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l19iQGjKGDOw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0)\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhjn4Wjv45-K"
      },
      "source": [
        "## CycleGAN generator\n",
        "\n",
        "Each CycleGAN generator has three sections\n",
        "\n",
        "- Encoder\n",
        "- Transformer\n",
        "- Decoder\n",
        "\n",
        "The input image is passed into the encoder. The encoder extracts features from the input image by using Convolutions and compressed the representation of image but increase the number of channels.\n",
        "\n",
        "The encoder consists of 3 convolution that reduces the representation by 1/4 th of actual image size. Consider an image of size (256, 256, 3) which we input into the encoder, the output of encoder will be (64, 64, 256).\n",
        "\n",
        "Then the output of encoder after activation function is applied is passed into the transformer. The transformer contains 6 or 9 residual blocks based on the size of input.\n",
        "\n",
        "The output of transformer is then passed into the decoder which uses 2 -deconvolution block of fraction strides to increase the size of representation to original size.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of generator is:\n",
        "\n",
        "`c7s1-64, d128, d256, R256, R256, R256,\n",
        "R256, R256, R256, u128, u64, c7s1-3\n",
        "\n",
        "where c7s1-k denote a 7×7 Convolution-InstanceNorm-ReLU layer with k filters and stride 1. dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2. Rk denotes a residual block that contains two 3 × 3 convolution layers with the same number of filters on both layer. uk denotes a 3 × 3 fractional-strides-Convolution-InstanceNorm-ReLU layer with k filters and stride 1/2 (i.e deconvolution operation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "alxDPbhV17qf"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        conv_block = [  nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features)  ]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, 64, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** i\n",
        "            model += [\n",
        "                nn.Conv2d(64 * mult, 64 * mult * 2, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(64 * mult * 2),\n",
        "            ]\n",
        "\n",
        "        mult = 2 ** n_downsampling\n",
        "        for i in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(64 * mult)]\n",
        "\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(64 * mult, 64 * mult // 2, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(64 * mult // 2),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(64, output_nc, 7)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhfSKwtOpIsN",
        "outputId": "e52de336-87b8-47a9-94b6-db97957e8d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator instantiation and basic tests passed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate generators\n",
        "gen_AtoB = Generator(3, 3)\n",
        "gen_BtoA = Generator(3, 3)\n",
        "\n",
        "# Basic tests with random input\n",
        "input_tensor = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "output_tensor = gen_AtoB(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256), \"Generator output has incorrect shape\"\n",
        "\n",
        "output_tensor = gen_BtoA(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256), \"Generator output has incorrect shape\"\n",
        "\n",
        "\n",
        "print(\"Generator instantiation and basic tests passed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8_IFKeJExbl"
      },
      "source": [
        "## CycleGAN Discriminator\n",
        "\n",
        "In discriminator the authors use PatchGAN discriminator. The difference between a PatchGAN and regular GAN discriminator is that rather the regular GAN maps from a 256×256 image to a single scalar output, which signifies “real” or “fake”, whereas the PatchGAN maps from 256×256 to an NxN (here 70×70) array of outputs X, where each Xij signifies whether the patch ij in the image is real or fake.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of discriminator is :\n",
        "\n",
        "`C64-C128-C256-C512`\n",
        "\n",
        "where Ck is 4×4 convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2. We don’t apply InstanceNorm on the first layer (C64). After the last layer, we apply convolution operation to produce a 1×1 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yg1GyEAe18kQ"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            self.discriminator_block(64, 128),\n",
        "            self.discriminator_block(128, 256),\n",
        "            self.discriminator_block(256, 512),\n",
        "\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def discriminator_block(self, input_dim, output_dim, is_first=False, is_last=False):\n",
        "        \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "        return nn.Sequential(\n",
        "                      nn.Conv2d(input_dim, output_dim, kernel_size=4, stride=2, padding=1),\n",
        "                      nn.InstanceNorm2d(output_dim),\n",
        "                      nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  self.model(x)\n",
        "        # Average pooling and flatten\n",
        "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0gm7fLqcZY",
        "outputId": "dfb4df65-7c75-4586-e5d1-cd1579df5f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discriminator instantiation and basic tests passed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate discriminators\n",
        "dis_A = Discriminator(3, 1)\n",
        "dis_B = Discriminator(3, 1)\n",
        "\n",
        "# Basic tests with random input\n",
        "input_tensor = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "output_tensor = dis_A(input_tensor)\n",
        "assert output_tensor.shape == (1, 1), \"Discriminator output has incorrect shape\"\n",
        "\n",
        "output_tensor = dis_B(input_tensor)\n",
        "assert output_tensor.shape == (1, 1), \"Discriminator output has incorrect shape\"\n",
        "\n",
        "print(\"Discriminator instantiation and basic tests passed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fseKuKXBjrLP"
      },
      "source": [
        "# Cost Functions\n",
        "\n",
        "- **Adversarial Loss:**  We apply adversarial loss to both our mappings of generators and discriminators. This adversary loss is written as :\n",
        "\n",
        "$$ Loss_{advers} \\left ( G, D_y, X, Y \\right ) =\\frac{1}{m}\\sum \\left ( 1 - D_y\\left ( G\\left ( x \\right ) \\right ) \\right )^{2} $$  \n",
        "\n",
        "$$ Loss_{advers}\\left ( F, D_x, Y, X \\right ) =\\frac{1}{m}\\sum \\left ( 1 - D_x\\left ( F\\left ( y \\right ) \\right ) \\right )^{2} $$   \n",
        "\n",
        "- **Cycle Consistency Loss:** Given a random set of images adversarial network can map the set of input image to random permutation of images in the output domain which may induce the output distribution similar to target distribution. Thus adversarial mapping cannot guarantee the input xi  to yi . For this to happen the author proposed that process should be cycle-consistent.\n",
        "\n",
        "  This loss function used in Cycle GAN to measure the error rate of  inverse mapping G(x) -> F(G(x)). The behavior induced by this loss function cause closely matching the real input (x) and F(G(x))\n",
        "\n",
        "$$ Loss_{cyc}\\left ( G, F, X, Y \\right ) =\\frac{1}{m}\\left [ \\left ( F\\left ( G\\left ( x_i \\right ) \\right )-x_i \\right ) +\\left ( G\\left ( F\\left ( y_i \\right ) \\right )-y_i \\right ) \\right ] $$   \n",
        "\n",
        "\n",
        "The Cost function we used is the sum of adversarial loss and cyclic consistent loss:\n",
        "\n",
        "\n",
        "$$ L\\left ( G, F, D_x, D_y \\right ) = L_{advers}\\left (G, D_y, X, Y \\right ) + L_{advers}\\left (F, D_x, Y, X \\right ) + \\lambda L_{cycl}\\left ( G, F, X, Y \\right ) $$\n",
        "\n",
        "and our aim is :\n",
        "\n",
        "\n",
        "$$ arg \\underset{G, F}{min}\\underset{D_x, D_y}{max}L\\left ( G, F, D_x, D_y \\right ) $$   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LDJzHyDbCnsW"
      },
      "outputs": [],
      "source": [
        "class CycleGANLoss(nn.Module):\n",
        "    \"\"\"Define different GAN objectives.\n",
        "\n",
        "    The CycleGANLoss class abstracts away the need to create the target label tensor\n",
        "    that has the same size as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_real_label=1.0, target_fake_label=0.0):\n",
        "        super(CycleGANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Create label tensors with the same size as the input.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            A label tensor filled with ground truth label, and with the size of the input\n",
        "        \"\"\"\n",
        "\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        target_tensor = self._get_target_tensor(prediction, target_is_real)\n",
        "        loss = self.loss(prediction, target_tensor)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ERKSpR0EiLM",
        "outputId": "53d3ff45-3816-4374-dbb3-286865441314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Optimizers, loss functions, and assert tests instantiated successfully.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate optimizers\n",
        "optimizer_G = optim.Adam(list(gen_AtoB.parameters()) + list(gen_BtoA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_A = optim.Adam(dis_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_B = optim.Adam(dis_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Instantiate loss functions\n",
        "criterionGAN = CycleGANLoss()\n",
        "criterionCycle = torch.nn.L1Loss()\n",
        "\n",
        "# Assert tests for dimensions, forward and backward pass, and test the losses calculations\n",
        "# Test generator forward pass\n",
        "input_tensor = torch.randn(1, 3, 256, 256)\n",
        "output_tensor = gen_AtoB(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256)\n",
        "\n",
        "# Test discriminator forward pass\n",
        "output_tensor = dis_A(input_tensor)\n",
        "assert output_tensor.shape == (1, 1)\n",
        "\n",
        "# Test backward pass for generator\n",
        "output_tensor = gen_AtoB(input_tensor)\n",
        "loss = criterionGAN(output_tensor, True)\n",
        "loss.backward()\n",
        "\n",
        "# Test backward pass for discriminator\n",
        "output_tensor = dis_A(input_tensor)\n",
        "loss = criterionGAN(output_tensor, True)\n",
        "loss.backward()\n",
        "\n",
        "# Test cycle consistency loss\n",
        "fake_B = gen_AtoB(input_tensor)\n",
        "rec_A = gen_BtoA(fake_B)\n",
        "cycle_loss = criterionCycle(rec_A, input_tensor)\n",
        "assert cycle_loss.shape == ()\n",
        "\n",
        "# Test GAN loss\n",
        "fake_B = gen_AtoB(input_tensor)\n",
        "pred_fake = dis_B(fake_B)\n",
        "gan_loss = criterionGAN(pred_fake, True)\n",
        "assert gan_loss.shape == ()\n",
        "\n",
        "print(\"Optimizers, loss functions, and assert tests instantiated successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CycleGAN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CycleGAN: \n",
        "    def __init__(self, input_nc, output_nc, lr=0.0002, beta1=0.5, beta2=0.999, device='cpu'):\n",
        "        \"\"\"\n",
        "        Initializes the CycleGAN model, optimizers, and losses.\n",
        "        \n",
        "        Args:\n",
        "        - input_nc: Number of input channels (e.g. 3 for RGB).\n",
        "        - output_nc: Number of output channels.\n",
        "        - lr: Learning rate for optimizers.\n",
        "        - beta1, beta2: Beta parameters for Adam optimizer.\n",
        "        - device: 'cuda' or 'cpu'.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        \n",
        "        # Initialize generators\n",
        "        self.gen_AtoB = Generator(input_nc, output_nc).to(self.device)\n",
        "        self.gen_BtoA = Generator(input_nc, output_nc).to(self.device)\n",
        "        \n",
        "        # Initialize discriminators\n",
        "        self.dis_A = Discriminator(input_nc).to(self.device)\n",
        "        self.dis_B = Discriminator(input_nc).to(self.device)\n",
        "        \n",
        "        # Define loss functions\n",
        "        self.adversarial_loss = CycleGANLoss().to(self.device)\n",
        "        self.cycle_loss    = nn.L1Loss().to(self.device)\n",
        "        self.identity_loss = nn.L1Loss().to(self.device)\n",
        "        \n",
        "        # Optimizers\n",
        "        self.optimizer_G = optim.Adam(list(self.gen_AtoB.parameters()) + list(self.gen_BtoA.parameters()), \n",
        "                                      lr=lr, betas=(beta1, beta2))\n",
        "        self.optimizer_D_A = optim.Adam(self.dis_A.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "        self.optimizer_D_B = optim.Adam(self.dis_B.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "    \n",
        "    def forward(self, real_A, real_B):\n",
        "        \"\"\"\n",
        "        Forward pass for both generators.\n",
        "        \n",
        "        Args:\n",
        "        - real_A: Real image from domain A.\n",
        "        - real_B: Real image from domain B.\n",
        "        \n",
        "        Returns:\n",
        "        - fake_B: Generated image for domain B.\n",
        "        - fake_A: Generated image for domain A.\n",
        "        - recovered_A: Reconstructed image from fake_B -> A.\n",
        "        - recovered_B: Reconstructed image from fake_A -> B.\n",
        "        \"\"\"\n",
        "        fake_B = self.gen_AtoB(real_A)\n",
        "        fake_A = self.gen_BtoA(real_B)\n",
        "        recovered_A = self.gen_BtoA(fake_B)\n",
        "        recovered_B = self.gen_AtoB(fake_A)\n",
        "        \n",
        "        return fake_B, fake_A, recovered_A, recovered_B\n",
        "    \n",
        "    def compute_loss(self, real_A, real_B):\n",
        "        \"\"\"\n",
        "        Computes the total loss for generators and discriminators.\n",
        "        \n",
        "        Args:\n",
        "        - real_A: Real image from domain A.\n",
        "        - real_B: Real image from domain B.\n",
        "        \n",
        "        Returns:\n",
        "        - loss_G: Generator total loss.\n",
        "        - loss_D_A: Discriminator A loss.\n",
        "        - loss_D_B: Discriminator B loss.\n",
        "        \"\"\"\n",
        "        # Forward pass for generators\n",
        "        fake_B, fake_A, recovered_A, recovered_B = self.forward(real_A, real_B)\n",
        "        \n",
        "        # Identity loss (G_A2B(B) should equal B, G_B2A(A) should equal A)\n",
        "        loss_identity_A = self.identity_loss(self.gen_BtoA(real_A), real_A)\n",
        "        loss_identity_B = self.identity_loss(self.gen_AtoB(real_B), real_B)\n",
        "\n",
        "        # GAN loss\n",
        "        loss_G_AtoB = self.adversarial_loss(self.dis_B(fake_B), torch.ones_like(self.dis_B(fake_B)))\n",
        "        loss_G_BtoA = self.adversarial_loss(self.dis_A(fake_A), torch.ones_like(self.dis_A(fake_A)))\n",
        "\n",
        "        # Cycle-consistency loss\n",
        "        loss_cycle_A = self.cycle_loss(recovered_A, real_A)\n",
        "        loss_cycle_B = self.cycle_loss(recovered_B, real_B)\n",
        "\n",
        "        # Total generator loss\n",
        "        loss_G = (loss_G_AtoB + loss_G_BtoA) + 10 * (loss_cycle_A + loss_cycle_B) + 5 * (loss_identity_A + loss_identity_B)\n",
        "\n",
        "        # Discriminator A loss\n",
        "        loss_real_A = self.adversarial_loss(self.dis_A(real_A), torch.ones_like(self.dis_A(real_A)))\n",
        "        loss_fake_A = self.adversarial_loss(self.dis_A(fake_A.detach()), torch.zeros_like(self.dis_A(fake_A)))\n",
        "        loss_D_A = (loss_real_A + loss_fake_A) * 0.5\n",
        "\n",
        "        # Discriminator B loss\n",
        "        loss_real_B = self.adversarial_loss(self.dis_B(real_B), torch.ones_like(self.dis_B(real_B)))\n",
        "        loss_fake_B = self.adversarial_loss(self.dis_B(fake_B.detach()), torch.zeros_like(self.dis_B(fake_B)))\n",
        "        loss_D_B = (loss_real_B + loss_fake_B) * 0.5\n",
        "        \n",
        "        return loss_G, loss_D_A, loss_D_B\n",
        "    \n",
        "    def optimize(self, real_A, real_B):\n",
        "        \"\"\"\n",
        "        Perform one optimization step for the generators and discriminators.\n",
        "        \n",
        "        Args:\n",
        "        - real_A: Real image from domain A.\n",
        "        - real_B: Real image from domain B.\n",
        "        \n",
        "        Returns:\n",
        "        - loss_G: Generator total loss.\n",
        "        - loss_D_A: Discriminator A loss.\n",
        "        - loss_D_B: Discriminator B loss.\n",
        "        \"\"\"\n",
        "        # Compute losses\n",
        "        loss_G, loss_D_A, loss_D_B = self.compute_loss(real_A, real_B)\n",
        "\n",
        "        # Optimize Generators\n",
        "        self.optimizer_G.zero_grad()\n",
        "        loss_G.backward()\n",
        "        self.optimizer_G.step()\n",
        "\n",
        "        # Optimize Discriminator A\n",
        "        self.optimizer_D_A.zero_grad()\n",
        "        loss_D_A.backward()\n",
        "        self.optimizer_D_A.step()\n",
        "\n",
        "        # Optimize Discriminator B\n",
        "        self.optimizer_D_B.zero_grad()\n",
        "        loss_D_B.backward()\n",
        "        self.optimizer_D_B.step()\n",
        "        \n",
        "        return loss_G.item(), loss_D_A.item(), loss_D_B.item()\n",
        "    \n",
        "    def save_model(self, epoch, path='cycle_gan_model.pth'):\n",
        "        \"\"\"\n",
        "        Save the current model state.\n",
        "        \n",
        "        Args:\n",
        "        - epoch: Current epoch number.\n",
        "        - path: Path to save the model.\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'gen_AtoB_state_dict': self.gen_AtoB.state_dict(),\n",
        "            'gen_BtoA_state_dict': self.gen_BtoA.state_dict(),\n",
        "            'dis_A_state_dict': self.dis_A.state_dict(),\n",
        "            'dis_B_state_dict': self.dis_B.state_dict(),\n",
        "            'optimizer_G_state_dict': self.optimizer_G.state_dict(),\n",
        "            'optimizer_D_A_state_dict': self.optimizer_D_A.state_dict(),\n",
        "            'optimizer_D_B_state_dict': self.optimizer_D_B.state_dict(),\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        \"\"\"\n",
        "        Load a saved model state.\n",
        "        \n",
        "        Args:\n",
        "        - path: Path to the saved model.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(path)\n",
        "        self.gen_AtoB.load_state_dict(checkpoint['gen_AtoB_state_dict'])\n",
        "        self.gen_BtoA.load_state_dict(checkpoint['gen_BtoA_state_dict'])\n",
        "        self.dis_A.load_state_dict(checkpoint['dis_A_state_dict'])\n",
        "        self.dis_B.load_state_dict(checkpoint['dis_B_state_dict'])\n",
        "        self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "        self.optimizer_D_A.load_state_dict(checkpoint['optimizer_D_A_state_dict'])\n",
        "        self.optimizer_D_B.load_state_dict(checkpoint['optimizer_D_B_state_dict'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbvH924Z9-mD"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TWmVGr01-AR-"
      },
      "outputs": [],
      "source": [
        "def show_img(img):\n",
        "    \"\"\"\n",
        "    Show image with it's size\n",
        "\n",
        "    img: tensor\n",
        "    \"\"\"\n",
        "\n",
        "    img = img.permute(1, 2, 0)\n",
        "    if img.shape[2]==1:\n",
        "        img = img.view(img.shape[0], img.shape[1])\n",
        "    plt.title(f'Image has size {img.cpu().numpy().shape}')\n",
        "    plt.imshow(img,cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEpKsZtqIlHX"
      },
      "outputs": [],
      "source": [
        "transform = v2.Compose([\n",
        "    v2.Resize((256, 256)),\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "dataset_A = datasets.ImageFolder('path_to_dataset_A', transform=transform)\n",
        "dataset_B = datasets.ImageFolder('path_to_dataset_B', transform=transform)\n",
        "\n",
        "# DataLoaders\n",
        "dataloader_A = DataLoader(dataset_A, batch_size=16, shuffle=True)\n",
        "dataloader_B = DataLoader(dataset_B, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p77qc0A-A6b"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZMSq_rfGLa-"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mtC3G1tuGRSh"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "\t\"latent_dim\" : 100,\n",
        "\t\"num_epochs\" : 100,\n",
        "\t\"batch_size\" : 16,\n",
        "\t\"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "\t\"lr\" : 0.0002,\n",
        "\t\"b1\" : 0.5,\n",
        "\t\"b2\" : 0.999,\n",
        "\t\"n_cpu\" : 8,\n",
        "\t\"img_size\" : 256,\n",
        "\t\"channels\" : 3,\n",
        "\t\"sample_interval\" : 100,\n",
        "\t\"checkpoint_interval\" : 10,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLfKoOJQGOqQ"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop includes the following steps:\n",
        "\n",
        "1. **Generate fake images** from the generators.\n",
        "2. **Compute adversarial loss** for both discriminators and generators.\n",
        "3. **Cycle loss** to ensure image reconstruction.\n",
        "4. **Identity loss** to preserve image identity during translation.\n",
        "5. Update the **discriminator and generator** weights using backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "zw9qxWKx-B-E",
        "outputId": "f3cd87f7-43bb-4255-ef41-329d169b5760"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "num_epochs = 100\n",
        "cycle_gan = CycleGAN(input_nc=3, output_nc=3, device=hyperparameters[\"device\"])\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_A, real_B) in enumerate(zip(dataloader_A, dataloader_B)):\n",
        "        real_A = real_A[0].to(hyperparameters[\"device\"])\n",
        "        real_B = real_B[0].to(hyperparameters[\"device\"])\n",
        "        \n",
        "        # Perform one optimization step\n",
        "        loss_G, loss_D_A, loss_D_B = cycle_gan.optimize(real_A, real_B)\n",
        "    \n",
        "    # Print progress\n",
        "    print(f\"[Epoch {epoch+1}/{num_epochs}] Loss_G: {loss_G:.4f}, Loss_D_A: {loss_D_A:.4f}, Loss_D_B: {loss_D_B:.4f}\")\n",
        "    \n",
        "    # Save model every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        cycle_gan.save_model(epoch)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
