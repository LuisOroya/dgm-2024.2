{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMh5APzXGI0j"
      },
      "source": [
        "# Implementing the CycleGAN (vanilla architecture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVYYbaJRGCWv"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "Reproduzir uma CycleGAN vanilla, baseada no artigo [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593v7).\n",
        "\n",
        "Mais especificamente, construir, treinar e documentar esta arquitetura de GAN utilizando Pytorch, baseado na implementação em [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l19iQGjKGDOw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0)\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhjn4Wjv45-K"
      },
      "source": [
        "## CycleGAN generator\n",
        "\n",
        "Each CycleGAN generator has three sections\n",
        "\n",
        "- Encoder\n",
        "- Transformer\n",
        "- Decoder\n",
        "\n",
        "The input image is passed into the encoder. The encoder extracts features from the input image by using Convolutions and compressed the representation of image but increase the number of channels.\n",
        "\n",
        "The encoder consists of 3 convolution that reduces the representation by 1/4 th of actual image size. Consider an image of size (256, 256, 3) which we input into the encoder, the output of encoder will be (64, 64, 256).\n",
        "\n",
        "Then the output of encoder after activation function is applied is passed into the transformer. The transformer contains 6 or 9 residual blocks based on the size of input.\n",
        "\n",
        "The output of transformer is then passed into the decoder which uses 2 -deconvolution block of fraction strides to increase the size of representation to original size.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of generator is:\n",
        "\n",
        "`c7s1-64, d128, d256, R256, R256, R256,\n",
        "R256, R256, R256, u128, u64, c7s1-3\n",
        "\n",
        "where c7s1-k denote a 7×7 Convolution-InstanceNorm-ReLU layer with k filters and stride 1. dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2. Rk denotes a residual block that contains two 3 × 3 convolution layers with the same number of filters on both layer. uk denotes a 3 × 3 fractional-strides-Convolution-InstanceNorm-ReLU layer with k filters and stride 1/2 (i.e deconvolution operation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "alxDPbhV17qf"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        conv_block = [  nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features)  ]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, 64, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** i\n",
        "            model += [\n",
        "                nn.Conv2d(64 * mult, 64 * mult * 2, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(64 * mult * 2),\n",
        "            ]\n",
        "\n",
        "        mult = 2 ** n_downsampling\n",
        "        for i in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(64 * mult)]\n",
        "\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(64 * mult, 64 * mult // 2, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(64 * mult // 2),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(64, output_nc, 7)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhfSKwtOpIsN",
        "outputId": "e52de336-87b8-47a9-94b6-db97957e8d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator instantiation and basic tests passed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate generators\n",
        "gen_AtoB = Generator(3, 3)\n",
        "gen_BtoA = Generator(3, 3)\n",
        "\n",
        "# Basic tests with random input\n",
        "input_tensor = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "output_tensor = gen_AtoB(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256), \"Generator output has incorrect shape\"\n",
        "\n",
        "output_tensor = gen_BtoA(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256), \"Generator output has incorrect shape\"\n",
        "\n",
        "\n",
        "print(\"Generator instantiation and basic tests passed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8_IFKeJExbl"
      },
      "source": [
        "## CycleGAN Discriminator\n",
        "\n",
        "In discriminator the authors use PatchGAN discriminator. The difference between a PatchGAN and regular GAN discriminator is that rather the regular GAN maps from a 256×256 image to a single scalar output, which signifies “real” or “fake”, whereas the PatchGAN maps from 256×256 to an NxN (here 70×70) array of outputs X, where each Xij signifies whether the patch ij in the image is real or fake.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of discriminator is :\n",
        "\n",
        "`C64-C128-C256-C512`\n",
        "\n",
        "where Ck is 4×4 convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2. We don’t apply InstanceNorm on the first layer (C64). After the last layer, we apply convolution operation to produce a 1×1 output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yg1GyEAe18kQ"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            self.discriminator_block(64, 128),\n",
        "            self.discriminator_block(128, 256),\n",
        "            self.discriminator_block(256, 512),\n",
        "\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def discriminator_block(self, input_dim, output_dim, is_first=False, is_last=False):\n",
        "        \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "        return nn.Sequential(\n",
        "                      nn.Conv2d(input_dim, output_dim, kernel_size=4, stride=2, padding=1),\n",
        "                      nn.InstanceNorm2d(output_dim),\n",
        "                      nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  self.model(x)\n",
        "        # Average pooling and flatten\n",
        "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0gm7fLqcZY",
        "outputId": "dfb4df65-7c75-4586-e5d1-cd1579df5f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discriminator instantiation and basic tests passed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate discriminators\n",
        "dis_A = Discriminator(3, 1)\n",
        "dis_B = Discriminator(3, 1)\n",
        "\n",
        "# Basic tests with random input\n",
        "input_tensor = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "output_tensor = dis_A(input_tensor)\n",
        "assert output_tensor.shape == (1, 1), \"Discriminator output has incorrect shape\"\n",
        "\n",
        "output_tensor = dis_B(input_tensor)\n",
        "assert output_tensor.shape == (1, 1), \"Discriminator output has incorrect shape\"\n",
        "\n",
        "print(\"Discriminator instantiation and basic tests passed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fseKuKXBjrLP"
      },
      "source": [
        "# Cost Functions\n",
        "\n",
        "- **Adversarial Loss:**  We apply adversarial loss to both our mappings of generators and discriminators. This adversary loss is written as :\n",
        "\n",
        "$$ Loss_{advers} \\left ( G, D_y, X, Y \\right ) =\\frac{1}{m}\\sum \\left ( 1 - D_y\\left ( G\\left ( x \\right ) \\right ) \\right )^{2} $$  \n",
        "\n",
        "$$ Loss_{advers}\\left ( F, D_x, Y, X \\right ) =\\frac{1}{m}\\sum \\left ( 1 - D_x\\left ( F\\left ( y \\right ) \\right ) \\right )^{2} $$   \n",
        "\n",
        "- **Cycle Consistency Loss:** Given a random set of images adversarial network can map the set of input image to random permutation of images in the output domain which may induce the output distribution similar to target distribution. Thus adversarial mapping cannot guarantee the input xi  to yi . For this to happen the author proposed that process should be cycle-consistent.\n",
        "\n",
        "  This loss function used in Cycle GAN to measure the error rate of  inverse mapping G(x) -> F(G(x)). The behavior induced by this loss function cause closely matching the real input (x) and F(G(x))\n",
        "\n",
        "$$ Loss_{cyc}\\left ( G, F, X, Y \\right ) =\\frac{1}{m}\\left [ \\left ( F\\left ( G\\left ( x_i \\right ) \\right )-x_i \\right ) +\\left ( G\\left ( F\\left ( y_i \\right ) \\right )-y_i \\right ) \\right ] $$   \n",
        "\n",
        "\n",
        "The Cost function we used is the sum of adversarial loss and cyclic consistent loss:\n",
        "\n",
        "\n",
        "$$ L\\left ( G, F, D_x, D_y \\right ) = L_{advers}\\left (G, D_y, X, Y \\right ) + L_{advers}\\left (F, D_x, Y, X \\right ) + \\lambda L_{cycl}\\left ( G, F, X, Y \\right ) $$\n",
        "\n",
        "and our aim is :\n",
        "\n",
        "\n",
        "$$ arg \\underset{G, F}{min}\\underset{D_x, D_y}{max}L\\left ( G, F, D_x, D_y \\right ) $$   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LDJzHyDbCnsW"
      },
      "outputs": [],
      "source": [
        "class CycleGANLoss(nn.Module):\n",
        "    \"\"\"Define different GAN objectives.\n",
        "\n",
        "    The CycleGANLoss class abstracts away the need to create the target label tensor\n",
        "    that has the same size as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_real_label=1.0, target_fake_label=0.0):\n",
        "        super(CycleGANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Create label tensors with the same size as the input.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            A label tensor filled with ground truth label, and with the size of the input\n",
        "        \"\"\"\n",
        "\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "        loss = self.loss(prediction, target_tensor)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ERKSpR0EiLM",
        "outputId": "53d3ff45-3816-4374-dbb3-286865441314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Optimizers, loss functions, and assert tests instantiated successfully.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate optimizers\n",
        "optimizer_G = optim.Adam(list(gen_AtoB.parameters()) + list(gen_BtoA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_A = optim.Adam(dis_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D_B = optim.Adam(dis_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Instantiate loss functions\n",
        "criterionGAN = CycleGANLoss()\n",
        "criterionCycle = torch.nn.L1Loss()\n",
        "\n",
        "# Assert tests for dimensions, forward and backward pass, and test the losses calculations\n",
        "# Test generator forward pass\n",
        "input_tensor = torch.randn(1, 3, 256, 256)\n",
        "output_tensor = gen_AtoB(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256)\n",
        "\n",
        "# Test discriminator forward pass\n",
        "output_tensor = dis_A(input_tensor)\n",
        "assert output_tensor.shape == (1, 1)\n",
        "\n",
        "# Test backward pass for generator\n",
        "output_tensor = gen_AtoB(input_tensor)\n",
        "loss = criterionGAN(output_tensor, True)\n",
        "loss.backward()\n",
        "\n",
        "# Test backward pass for discriminator\n",
        "output_tensor = dis_A(input_tensor)\n",
        "loss = criterionGAN(output_tensor, True)\n",
        "loss.backward()\n",
        "\n",
        "# Test cycle consistency loss\n",
        "fake_B = gen_AtoB(input_tensor)\n",
        "rec_A = gen_BtoA(fake_B)\n",
        "cycle_loss = criterionCycle(rec_A, input_tensor)\n",
        "assert cycle_loss.shape == ()\n",
        "\n",
        "# Test GAN loss\n",
        "fake_B = gen_AtoB(input_tensor)\n",
        "pred_fake = dis_B(fake_B)\n",
        "gan_loss = criterionGAN(pred_fake, True)\n",
        "assert gan_loss.shape == ()\n",
        "\n",
        "print(\"Optimizers, loss functions, and assert tests instantiated successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbvH924Z9-mD"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TWmVGr01-AR-"
      },
      "outputs": [],
      "source": [
        "def show_img(img):\n",
        "    \"\"\"\n",
        "    Show image with it's size\n",
        "\n",
        "    img: tensor\n",
        "    \"\"\"\n",
        "\n",
        "    img = img.permute(1, 2, 0)\n",
        "    if img.shape[2]==1:\n",
        "        img = img.view(img.shape[0], img.shape[1])\n",
        "    plt.title(f'Image has size {img.cpu().numpy().shape}')\n",
        "    plt.imshow(img,cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEpKsZtqIlHX"
      },
      "outputs": [],
      "source": [
        "transforms_all = v2.Compose([\n",
        "    v2.Resize(size=50), # Resize the input to the size (50,50).\n",
        "    v2.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p77qc0A-A6b"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZMSq_rfGLa-"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mtC3G1tuGRSh"
      },
      "outputs": [],
      "source": [
        "latent_dim = 100\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "img_size = 256\n",
        "channels = 3\n",
        "sample_interval = 100\n",
        "checkpoint_interval = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLfKoOJQGOqQ"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "zw9qxWKx-B-E",
        "outputId": "f3cd87f7-43bb-4255-ef41-329d169b5760"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "#for epoch in range(num_epochs):\n",
        "#    for i, batch in enumerate(dataloader):\n",
        "#       # Convert list to tensor\n",
        "#        real_images = batch[0].to(device)\n",
        "#        # Adversarial ground truths\n",
        "#        valid = torch.ones(real_images.size(0), 1, device=device)\n",
        "#        fake = torch.zeros(real_images.size(0), 1, device=device)\n",
        "#        # Configure input\n",
        "#        real_images = real_images.to(device)\n",
        "#\n",
        "#        # ---------------------\n",
        "#        #  Train Discriminator\n",
        "#        # ---------------------\n",
        "#        optimizer_D.zero_grad()\n",
        "#        # Sample noise as generator input\n",
        "#        z = torch.randn(real_images.size(0), latent_dim, device=device)\n",
        "#        # Generate a batch of images\n",
        "#        fake_images = generator(z)\n",
        "#\n",
        "#        # Measure discriminator's ability\n",
        "#        # to classify real and fake images\n",
        "#        real_loss = adversarial_loss(discriminator\\\n",
        "#                                     (real_images), valid)\n",
        "#        fake_loss = adversarial_loss(discriminator\\\n",
        "#                                     (fake_images.detach()), fake)\n",
        "#        d_loss = (real_loss + fake_loss) / 2\n",
        "#        # Backward pass and optimize\n",
        "#        d_loss.backward()\n",
        "#        optimizer_D.step()\n",
        "#\n",
        "#        # -----------------\n",
        "#        #  Train Generator\n",
        "#        # -----------------\n",
        "#\n",
        "#        optimizer_G.zero_grad()\n",
        "#        # Generate a batch of images\n",
        "#        gen_images = generator(z)\n",
        "#        # Adversarial loss\n",
        "#        g_loss = adversarial_loss(discriminator(gen_images), valid)\n",
        "#        # Backward pass and optimize\n",
        "#        g_loss.backward()\n",
        "#        optimizer_G.step()\n",
        "#        # ---------------------\n",
        "#        #  Progress Monitoring\n",
        "#        # ---------------------\n",
        "#        if (i + 1) % 100 == 0:\n",
        "#            print(\n",
        "#                f\"Epoch [{epoch+1}/{num_epochs}]\\\n",
        "#                        Batch {i+1}/{len(dataloader)} \"\n",
        "#                f\"Discriminator Loss: {d_loss.item():.4f} \"\n",
        "#                f\"Generator Loss: {g_loss.item():.4f}\"\n",
        "#            )\n",
        "#    # Save generated images for every epoch\n",
        "#    if (epoch + 1) % 10 == 0:\n",
        "#        with torch.no_grad():\n",
        "#            z = torch.randn(16, latent_dim, device=device)\n",
        "#            generated = generator(z).detach().cpu()\n",
        "#            grid = torchvision.utils.make_grid(generated,\\\n",
        "#                                        nrow=4, normalize=True)\n",
        "#            plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "#            plt.axis(\"off\")\n",
        "#            plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
