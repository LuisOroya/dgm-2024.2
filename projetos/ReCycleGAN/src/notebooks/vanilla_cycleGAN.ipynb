{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the CycleGAN (vanilla architecture)"
      ],
      "metadata": {
        "id": "xMh5APzXGI0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo\n",
        "\n",
        "Reproduzir uma CycleGAN vanilla, baseada no artigo [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593v7).\n",
        "\n",
        "Mais especificamente, construir, treinar e documentar esta arquitetura de GAN utilizando Pytorch, baseado na implementação em [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py)."
      ],
      "metadata": {
        "id": "LVYYbaJRGCWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0)\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "l19iQGjKGDOw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CycleGAN generator\n",
        "\n",
        "Each CycleGAN generator has three sections\n",
        "\n",
        "- Encoder\n",
        "- Transformer\n",
        "- Decoder\n",
        "\n",
        "The input image is passed into the encoder. The encoder extracts features from the input image by using Convolutions and compressed the representation of image but increase the number of channels.\n",
        "\n",
        "The encoder consists of 3 convolution that reduces the representation by 1/4 th of actual image size. Consider an image of size (256, 256, 3) which we input into the encoder, the output of encoder will be (64, 64, 256).\n",
        "\n",
        "Then the output of encoder after activation function is applied is passed into the transformer. The transformer contains 6 or 9 residual blocks based on the size of input.\n",
        "\n",
        "The output of transformer is then passed into the decoder which uses 2 -deconvolution block of fraction strides to increase the size of representation to original size.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of generator is:\n",
        "\n",
        "`c7s1-64, d128, d256, R256, R256, R256,\n",
        "R256, R256, R256, u128, u64, c7s1-3\n",
        "\n",
        "where c7s1-k denote a 7×7 Convolution-InstanceNorm-ReLU layer with k filters and stride 1. dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2. Rk denotes a residual block that contains two 3 × 3 convolution layers with the same number of filters on both layer. uk denotes a 3 × 3 fractional-strides-Convolution-InstanceNorm-ReLU layer with k filters and stride 1/2 (i.e deconvolution operation)."
      ],
      "metadata": {
        "id": "mhjn4Wjv45-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        conv_block = [  nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.ReflectionPad2d(1),\n",
        "                        nn.Conv2d(in_features, in_features, 3),\n",
        "                        nn.InstanceNorm2d(in_features)  ]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, 64, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** i\n",
        "            model += [\n",
        "                nn.Conv2d(64 * mult, 64 * mult * 2, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(64 * mult * 2),\n",
        "            ]\n",
        "\n",
        "        mult = 2 ** n_downsampling\n",
        "        for i in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(64 * mult)]\n",
        "\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(64 * mult, 64 * mult // 2, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(64 * mult // 2),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "\n",
        "        model += [nn.ReflectionPad2d(3)]\n",
        "        model += [nn.Conv2d(64, output_nc, 7)]\n",
        "        model += [nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "metadata": {
        "id": "alxDPbhV17qf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate generators\n",
        "gen_AtoB = Generator(3, 3)\n",
        "gen_BtoA = Generator(3, 3)\n",
        "\n",
        "# Basic tests with random input\n",
        "input_tensor = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "output_tensor = gen_AtoB(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256), \"Generator output has incorrect shape\"\n",
        "\n",
        "output_tensor = gen_BtoA(input_tensor)\n",
        "assert output_tensor.shape == (1, 3, 256, 256), \"Generator output has incorrect shape\"\n",
        "\n",
        "\n",
        "print(\"Generator instantiation and basic tests passed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhfSKwtOpIsN",
        "outputId": "e3713cab-8479-459d-8ef7-29e1ede636e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator instantiation and basic tests passed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CycleGAN Discriminator\n",
        "\n",
        "In discriminator the authors use PatchGAN discriminator. The difference between a PatchGAN and regular GAN discriminator is that rather the regular GAN maps from a 256×256 image to a single scalar output, which signifies “real” or “fake”, whereas the PatchGAN maps from 256×256 to an NxN (here 70×70) array of outputs X, where each Xij signifies whether the patch ij in the image is real or fake.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of discriminator is :\n",
        "\n",
        "`C64-C128-C256-C512`\n",
        "\n",
        "where Ck is 4×4 convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2. We don’t apply InstanceNorm on the first layer (C64). After the last layer, we apply convolution operation to produce a 1×1 output."
      ],
      "metadata": {
        "id": "P8_IFKeJExbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            self.discriminator_block(64, 128),\n",
        "            self.discriminator_block(128, 256),\n",
        "            self.discriminator_block(256, 512),\n",
        "\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def discriminator_block(self, input_dim, output_dim, is_first=False, is_last=False):\n",
        "        \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "        return nn.Sequential(\n",
        "                      nn.Conv2d(input_dim, output_dim, kernel_size=4, stride=2, padding=1),\n",
        "                      nn.InstanceNorm2d(output_dim),\n",
        "                      nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  self.model(x)\n",
        "        # Average pooling and flatten\n",
        "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
      ],
      "metadata": {
        "id": "Yg1GyEAe18kQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate discriminators\n",
        "dis_A = Discriminator(3, 1)  # Assuming input channels are 3 (RGB) and output channels are 1 (real/fake)\n",
        "dis_B = Discriminator(3, 1)\n",
        "\n",
        "# Basic tests with random input\n",
        "input_tensor = torch.randn(1, 3, 256, 256)  # Example input tensor (batch size 1, 3 channels, 256x256 image)\n",
        "\n",
        "output_tensor = dis_A(input_tensor)\n",
        "assert output_tensor.shape == (1, 1), \"Discriminator output has incorrect shape\"\n",
        "\n",
        "output_tensor = dis_B(input_tensor)\n",
        "assert output_tensor.shape == (1, 1), \"Discriminator output has incorrect shape\"\n",
        "\n",
        "print(\"Discriminator instantiation and basic tests passed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0gm7fLqcZY",
        "outputId": "9093d896-ef70-4119-9703-c58052faf810"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator instantiation and basic tests passed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cost Functions\n",
        "\n",
        "- **Adversarial Loss:**  We apply adversarial loss to both our mappings of generators and discriminators. This adversary loss is written as :\n",
        "\n",
        "$$ Loss_{advers} \\left ( G, D_y, X, Y \\right ) =\\frac{1}{m}\\sum \\left ( 1 - D_y\\left ( G\\left ( x \\right ) \\right ) \\right )^{2} $$  \n",
        "\n",
        "$$ Loss_{advers}\\left ( F, D_x, Y, X \\right ) =\\frac{1}{m}\\sum \\left ( 1 - D_x\\left ( F\\left ( y \\right ) \\right ) \\right )^{2} $$   \n",
        "\n",
        "- **Cycle Consistency Loss:** Given a random set of images adversarial network can map the set of input image to random permutation of images in the output domain which may induce the output distribution similar to target distribution. Thus adversarial mapping cannot guarantee the input xi  to yi . For this to happen the author proposed that process should be cycle-consistent.\n",
        "\n",
        "  This loss function used in Cycle GAN to measure the error rate of  inverse mapping G(x) -> F(G(x)). The behavior induced by this loss function cause closely matching the real input (x) and F(G(x))\n",
        "\n",
        "$$ Loss_{cyc}\\left ( G, F, X, Y \\right ) =\\frac{1}{m}\\left [ \\left ( F\\left ( G\\left ( x_i \\right ) \\right )-x_i \\right ) +\\left ( G\\left ( F\\left ( y_i \\right ) \\right )-y_i \\right ) \\right ] $$   \n",
        "\n",
        "\n",
        "The Cost function we used is the sum of adversarial loss and cyclic consistent loss:\n",
        "\n",
        "\n",
        "$$ L\\left ( G, F, D_x, D_y \\right ) = L_{advers}\\left (G, D_y, X, Y \\right ) + L_{advers}\\left (F, D_x, Y, X \\right ) + \\lambda L_{cycl}\\left ( G, F, X, Y \\right ) $$\n",
        "\n",
        "and our aim is :\n",
        "\n",
        "\n",
        "$$ arg \\underset{G, F}{min}\\underset{D_x, D_y}{max}L\\left ( G, F, D_x, D_y \\right ) $$   "
      ],
      "metadata": {
        "id": "fseKuKXBjrLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGANLoss():\n",
        "    def __init__(self, lambda_cyc=10):\n",
        "        self.lambda_cyc = lambda_cyc\n",
        "        self.criterionGAN = nn.MSELoss()\n",
        "        self.criterionCycle = nn.L1Loss()\n",
        "\n",
        "    def adversarial_loss(self, D_output):\n",
        "        return self.criterionGAN(D_output, torch.ones_like(D_output))\n",
        "\n",
        "    def cycle_consistency_loss(self, real_img, reconstructed_img):\n",
        "        return self.criterionCycle(reconstructed_img, real_img) * self.lambda_cyc\n",
        "\n",
        "    def calculate_loss(self, Dx_output_fake, Dy_output_fake, real_x, real_y, reconstructed_x, reconstructed_y):\n",
        "        # Adversarial loss\n",
        "        adv_loss_G = self.adversarial_loss(Dy_output_fake)\n",
        "        adv_loss_F = self.adversarial_loss(Dx_output_fake)\n",
        "\n",
        "        # Cycle consistency loss\n",
        "        cyc_loss_G = self.cycle_consistency_loss(real_x, reconstructed_x)\n",
        "        cyc_loss_F = self.cycle_consistency_loss(real_y, reconstructed_y)\n",
        "\n",
        "        # Total generator loss\n",
        "        total_gen_loss = adv_loss_G + adv_loss_F + cyc_loss_G + cyc_loss_F\n",
        "\n",
        "        return total_gen_loss\n"
      ],
      "metadata": {
        "id": "L9vTeyzHjsbC"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}